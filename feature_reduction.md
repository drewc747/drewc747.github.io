## Feature Reduction

**Techniques Explored:** Principle Componant Analysis (PCA), Linear Discriminant Analysis (LDA), Multi-Dimensional Scaling (MDS), Locality Sensitive Hashing (LSH)

**Skills Demonstrated:** Python, Feature Reduction Techniques

**Project description:** In machine learning it is often desired to reduce the amount of features (or dimensions) of a given problem. As the number of features in a problem increases, so does the problem complexity. However, it is often the case the features are redundant and can be removed with minimal decrease in a models performance while significantly decreasing the models complexity. This project will first explore the curse of dimensionality, a simple projection example, and then implament some common techniques used for feature reduction as well as the advantages and disadvantages of each. Techniques to be explored are PCA, LDA, MDS, and LSH. 

### 1. What is feature reduction? A simple example:

Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. 

```python
if (isAwesome){
  return true
}
```

### 2. Why is feature reduction important? The curse of dimensionality:


### 3. Principle Componant Analysis (PCA)

<img src="images/dummy_thumbnail.jpg?raw=true"/>

### 4. Linear Discriminant Analysis (LDA)


### 5. Multi-Dimensional Scaling (MDS)


### 6. Locality Sensitive Hashing (LSH)

For all code, see [GitHub Repository](https://github.com/drewc747/machine-learning-examples/tree/master/feature_reduction)
