## Feature Reduction

**Techniques Explored:** Principle Componant Analysis (PCA), Linear Discriminant Analysis (LDA), Multi-Dimensional Scaling (MDS), Locality Sensitive Hashing (LSH)

**Skills Demonstrated:** Python, Feature Reduction Techniques

**Project description:** In machine learning it is often desired to reduce the amount of features (or dimensions) of a given problem. As the number of features in a problem increases, so does the problem complexity. However, it is often the case the features are redundant and can be removed with minimal decrease in a models performance while significantly decreasing the models complexity. This project will first explore the curse of dimensionality, a simple projection example, and then implament some common techniques used for feature reduction as well as the advantages and disadvantages of each. Techniques to be explored are PCA, LDA, MDS, and LSH. 

### 1. What is feature reduction? A simple example:
As a simple example, let look at training data in a 2D space consisting of blue circles and orange circles. The data is generated by creating seperate 2-D gaussian scatter plots. The code to create the data and plot of the data is shown below:

```python
import numpy as np
import matplotlib.pyplot.plt

def plot_scatters(scatter_list, scatter_names):
    '''
    Method to plot a multiple scatter plots from a list
        scatter_list: List of lists, each list should contain scatter data (x, y)
        scatter_names: List of names for each scatter list to be plotted
    '''
    fig, ax = plt.subplots(figsize=(7, 7))
    for s in range(len(scatter_list)):
        ax.scatter(scatter_list[s][:,0], scatter_list[s][:,1], s=10.0, alpha=0.3, label=scatter_names[s])
    
    # Create x and y axis lines    
    ax.spines['left'].set_position('zero')
    ax.spines['right'].set_color('none')
    ax.spines['bottom'].set_position('zero')
    ax.spines['top'].set_color('none')
    ax.spines['left'].set_smart_bounds(True)
    ax.spines['bottom'].set_smart_bounds(True)
    
    ax.legend(loc=0)
    plt.show()
    
def generate_gaussian_scatter(mean, cov, num_samp):
    '''
    Method to generate a gaussian scatter
        mean: List of x and y values for mean
        cov: List of lists representing the covariance matrix, 2x2 matrix
        num_samp: Number of samples
    '''
    np.random.seed(1024)
    gaussian_scatter = np.random.multivariate_normal(mean, cov, num_samp)
    return gaussian_scatter

def projection_example():
    scatter_list = []
    scatter_list.append(generate_gaussian_scatter(mean = [-5,5], cov = [[1, 1.5], [1.5, 5]], num_samp = 1000))
    scatter_list.append(generate_gaussian_scatter(mean = [5,3], cov = [[1, 1.5], [1.5, 5]], num_samp = 1000))
    
    scatter_names = ["2D-Gaussian Blue", "2D-Gaussian Set Orange"]
    
    plot_scatters(scatter_list, scatter_names)
    
def main():
    projection_example()

if __name__ == '__main__':
    main()
}
```

The code is a bit overkill for this example but is meant to be scalable for more complex examples and will serve as the base code for the rest of this tutorial. For this simple example we will be working off of the "projection_example" method. Here we create two 2-D gaussian scatter plots. The data is created using the "generate_gaussian_scatter()" method which takes a mean, covariance matrix, and number of samples and uses the numpy package to create a random list of data using the Gaussian. These lists are then fed into the "plot_scatters()" definition which provides the plot below:

<img src = "images/gaussian_scatter.png" />

Based on this training data, we may want to create a classifier that can tell us whether a data point blue or orange based on the xy coordinates, and in the case above there is a clear correlation with xy coordinates and color. We could draw a line between to seperate the training data, and then use that line for the test data to make a prediction on whether or not a data point is blue or orange based on where the datapoint falls relative to the line. Without getting into advanced techniques, we may just naiively choose any arbitrary line that seperates the two classes like shown in the plot below.

<img src = "images/gaussian_scatter_arb_line.png" />

Using this line we can now predict what color future data points are based on their x,y corridinates. If the point falls to the left of the line, we may predict it to be blue, otherwise we would predict it to be orange.

Upon closer inspection, you can see that if you were to project all the data onto the x-axis, there is still seperation between the data points suggesting the the only feature that is needed to classify this data set is the x-coordinate. The plot below shows the data points projected onto the x axis and a point on the x-axis used as a threshold for classification.

<img src = "images/gaussian_scatter_x_proj.png" />

Alternatively, if we try projecting the data onto the y-axis, we no longer have the ability to distinguish the red circles and blue square based on just the y value.  The plot below shows the data projected onto the y-axis.

<img src = "images/gaussian_scatter_y_proj.png" />

This simple example shows that feature reduction can be performed in order to simplify a problem by removing redundant features. In this case, the y-value is redundant and unecessary for the classification of red circles and blue squares.


### 2. Why is feature reduction important? The curse of dimensionality:

The simple example above shows what feature reduction is, but typically a 2D problem isn't a problem we would need to perform feature reduction. However, most machine learning problems deal with high dimensions, which is hard to visualize and isn't as simple as plotting the data and picking a line to seperate the classes. In this section, we'll explore the curse of dimensionality and show why feature reduction is so important for higher dimensional problems.

The curse of dimensionality refers to various phenomona that occurs when increasing dimensions. One such phenomona is that as dimensions increase, dimension space increases exponentially. This can be imagined in the projection example above. The data can be represented on the x-axis with a dimensional space of 15 (from -7.5 to 7.5), but when the y-axis is taken into account it is represented in the x range by 15 and the y range by 17.5 (-5 to 12.5), giving a total dimensional space of 15*17.5 = 262.5. So by removing the redundant y value, we reduce the dimensional space from 262.5 to 15. In general, as the amount of features increases, the requried dimensional space increases exponentially.

Another phenomona is sparsity, since dimensional space increases exponentially, but the number of samples remains a constant, the area between samples increases exponentially as well which can make it more difficult to group data and efficiently generalize. As the features continue to increase without increasing the amount of training samples, there is a higher opporunity to overfit. In order to explore this problem in more detail, we will use the Gaussian distribution in high dimensional space. A spherical Gaussian in *m* dimensions can be explained by the following equation:

&ensp;&ensp;&ensp;<img src = "images/gaussian_m_dimensions_eqn.png"/>

We define an *m*-dimensional sphere as a set of points in *m*-dimensional space that a distance *r* from the origin.

&ensp;&ensp;&ensp;<img src = "images/sphere_m_dimensions_eqn.png"/>

Where *S*<sub>*m-1*</sub>*(r)* represents the surface area of the *m*-dimensional sphere. The integral of the surface area gives us the volume of the sphere, or alternatively, the surface area can be expressed as the derivative of the volume:

&ensp;&ensp;&ensp;<img src = "images/sphere_surface_area_eqn.png"/>

Given that the volume of an *m*-dimensional circle is represented by:

&ensp;&ensp;&ensp;<img src = "images/sphere_m_dim_volume_eqn.png"/>

Where *C* is some constant, and *r* is the radius of the sphere, we can then perform the derivative to get *S*<sub>*m-1*</sub>*(r)*:

&ensp;&ensp;&ensp;<img src = "images/sphere_surface_area_r_eqn.png"/>

If we look at a unit sphere (*r* = 1), the surface area of a unit circle simplifies to:

&ensp;&ensp;&ensp;<img src = "images/unit_sphere_surface_area_eqn.png"/>

Substituting this in for our equation of the surface area of any *m*-dimensional sphere yields:

&ensp;&ensp;&ensp;<img src = "images/sphere_surface_area_r_unit_sphere_eqn.png" />

We can then calculate the density of the sampled points using the above equations. For any *x* lying on the sphere with radius *r*, the probability density is the same as ||*x*||<sub>2</sub> = *r*. Therefore we can directly multiply the probability density of every point by the surface area and get:

&ensp;&ensp;&ensp;<img src = "images/sphere_density_eqn.png" />

We can then calculate the radius at which the density has a single maximum value, *r_hat* for large *m* by taking the gradient with respect to *r* to be zero and solve for *r_hat*.

&ensp;&ensp;&ensp;<img src = "images/r_hat_sol_1.png" />

&ensp;&ensp;&ensp;<img src = "images/r_hat_sol_2.png" />

&ensp;&ensp;&ensp;<img src = "images/r_hat_sol_3.png" />

For large *m*, we now consider a small value *epsilon* << *r_hat* and consider to the density ratio of *r_hat + epsilon* over *r_hat*:

&ensp;&ensp;&ensp;<img src = "images/density_ratio_sol_1.png" />

&ensp;&ensp;&ensp;<img src = "images/density_ratio_sol_2.png" />

&ensp;&ensp;&ensp;<img src = "images/density_ratio_sol_3.png" />

Using Taylor expansion for the log function, we get the following approximation:

&ensp;&ensp;&ensp;<img src = "images/density_ratio_sol_4.png" />

&ensp;&ensp;&ensp;<img src = "images/density_ratio_sol_5.png" />

And finally we are left with the approximated density ratio for large *m*:

&ensp;&ensp;&ensp;<img src = "images/density_ratio_sol_6.png" />

Putting this all together we see that for a high dimensional gaussian, the greatest density of samples will reside near *r_hat* = *sqrt(m-1) * sigma*, and as you move away from that radius by some distance *epsilon*, the density will decrease exponentially as *exp*(-*epsilon*<sup>*2*</sup>/*sigma*<sup>*2*</sup>). However, in low dimensions, for example 2D and 3D, we see the majority of the samples reside near the origin within *sigma*.

We can write a quick python script that samples from an m-dimensional Gaussian to show this. In the script below we produce 100 samples and compute the mean and standard deviation of the radius of the samples for each value *m* from 1 to 500.

```python
num_samples = 100
dim = range(1, 500)

r_mean = []
r_sigma = []

for d in dim:
    # Generate m-dimensional gaussian points and calculate radius and sigma
    normal_deviates = np.random.normal(size=(d, num_samples))
    radius = np.sqrt((normal_deviates**2).sum(axis=0))
    sigma = np.std(radius)
    
    r_mean.append(np.mean(radius))
    r_sigma.append(sigma)

fig, ax = plt.subplots(figsize=(7, 7))
ax.plot(dim, r_mean)
ax.set(xlabel='Dimensions', ylabel='Radius mean', title='Radius vs dimensions - 100 samples')
plt.show

fig, ax = plt.subplots(figsize=(7, 7))
ax.plot(dim,r_sigma)
ax.set(xlabel='Dimensions', ylabel='Sigma', title='Sigma vs. dimensions - 100 samples')
plt.show()
```

&ensp;&ensp;&ensp;<img src = "images/curse_of_dim_radius.png" />

&ensp;&ensp;&ensp;<img src = "images/curse_of_dim_sigma.png" />

The plots above agree with the formulas we derived, as *m* is increased, the average radius in creases as the *sqrt(m) * sigma*. Since *sigma* doesn't increase, the majority of the points stay near the radius and the majority of the points live only on the "shell" of the *m*-dimensional sphere as opposed to the entire volume.

### 3. Principle Componant Analysis (PCA)

<img src="images/dummy_thumbnail.jpg?raw=true"/>

### 4. Linear Discriminant Analysis (LDA)


### 5. Multi-Dimensional Scaling (MDS)


### 6. Locality Sensitive Hashing (LSH)

For all code, see [GitHub Repository](https://github.com/drewc747/machine-learning-examples/tree/master/feature_reduction)
